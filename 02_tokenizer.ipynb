{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:56:44.109922200Z",
     "start_time": "2024-08-06T06:56:33.735377500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "yazi = \"ahmet\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:56:44.115001600Z",
     "start_time": "2024-08-06T06:56:44.109922200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'a', 'e', 'h', 'm', 't'}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(yazi)\n",
    "vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:56:44.125431300Z",
     "start_time": "2024-08-06T06:56:44.115921800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['a', 'e', 'h', 'm', 't']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2= sorted(list(vocab))\n",
    "vocab2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:56:56.091968500Z",
     "start_time": "2024-08-06T06:56:56.065302700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'a')\n",
      "(1, 'e')\n",
      "(2, 'h')\n",
      "(3, 'm')\n",
      "(4, 't')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(vocab2):\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:56:56.951675400Z",
     "start_time": "2024-08-06T06:56:56.945286500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'e': 1, 'h': 2, 'm': 3, 't': 4}\n"
     ]
    }
   ],
   "source": [
    "abc = {karakter:id for id, karakter in enumerate(vocab2)}\n",
    "print(abc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:56:57.451477400Z",
     "start_time": "2024-08-06T06:56:57.441176300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:56:58.034785400Z",
     "start_time": "2024-08-06T06:56:57.863763600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "text = Path(\"tiny-shakespeare.txt\").read_text()\n",
    "print(text[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:56:58.723707300Z",
     "start_time": "2024-08-06T06:56:58.715551300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self,vocabulary):\n",
    "        self.token_id_for_char = {char:token_id for token_id, char in enumerate(vocabulary)}\n",
    "        self.char_for_token_id = {token_id:char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "    @staticmethod\n",
    "    def train_from_text(text):\n",
    "        vocabulary = set(text)\n",
    "        return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "    def encode(self,text):\n",
    "        token_ids = []\n",
    "        for char in text:\n",
    "            token_ids.append(self.token_id_for_char[char])\n",
    "        return torch.tensor(token_ids,dtype=torch.long)\n",
    "\n",
    "    def decode(self,token_ids):\n",
    "        chars =[]\n",
    "        for id in token_ids.tolist():\n",
    "            chars.append(self.char_for_token_id[id])\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        return len(self.token_id_for_char)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:41:57.705693300Z",
     "start_time": "2024-08-06T10:41:57.690220Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:41:58.185814300Z",
     "start_time": "2024-08-06T10:41:58.157448400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:41:58.445126200Z",
     "start_time": "2024-08-06T10:41:58.441629500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 46, 51, 43, 58])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Ahmet\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:41:58.898082600Z",
     "start_time": "2024-08-06T10:41:58.889216800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world Ahmet!!\n"
     ]
    }
   ],
   "source": [
    "tokenized_text01 = tokenizer.encode(\"Hello world Ahmet!!\")\n",
    "print(tokenizer.decode(tokenized_text01))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:41:59.178787900Z",
     "start_time": "2024-08-06T10:41:59.174334300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n',\n",
      " 1: ' ',\n",
      " 2: '!',\n",
      " 3: '$',\n",
      " 4: '&',\n",
      " 5: \"'\",\n",
      " 6: ',',\n",
      " 7: '-',\n",
      " 8: '.',\n",
      " 9: '3',\n",
      " 10: ':',\n",
      " 11: ';',\n",
      " 12: '?',\n",
      " 13: 'A',\n",
      " 14: 'B',\n",
      " 15: 'C',\n",
      " 16: 'D',\n",
      " 17: 'E',\n",
      " 18: 'F',\n",
      " 19: 'G',\n",
      " 20: 'H',\n",
      " 21: 'I',\n",
      " 22: 'J',\n",
      " 23: 'K',\n",
      " 24: 'L',\n",
      " 25: 'M',\n",
      " 26: 'N',\n",
      " 27: 'O',\n",
      " 28: 'P',\n",
      " 29: 'Q',\n",
      " 30: 'R',\n",
      " 31: 'S',\n",
      " 32: 'T',\n",
      " 33: 'U',\n",
      " 34: 'V',\n",
      " 35: 'W',\n",
      " 36: 'X',\n",
      " 37: 'Y',\n",
      " 38: 'Z',\n",
      " 39: 'a',\n",
      " 40: 'b',\n",
      " 41: 'c',\n",
      " 42: 'd',\n",
      " 43: 'e',\n",
      " 44: 'f',\n",
      " 45: 'g',\n",
      " 46: 'h',\n",
      " 47: 'i',\n",
      " 48: 'j',\n",
      " 49: 'k',\n",
      " 50: 'l',\n",
      " 51: 'm',\n",
      " 52: 'n',\n",
      " 53: 'o',\n",
      " 54: 'p',\n",
      " 55: 'q',\n",
      " 56: 'r',\n",
      " 57: 's',\n",
      " 58: 't',\n",
      " 59: 'u',\n",
      " 60: 'v',\n",
      " 61: 'w',\n",
      " 62: 'x',\n",
      " 63: 'y',\n",
      " 64: 'z'}\n",
      "{'\\n': 0,\n",
      " ' ': 1,\n",
      " '!': 2,\n",
      " '$': 3,\n",
      " '&': 4,\n",
      " \"'\": 5,\n",
      " ',': 6,\n",
      " '-': 7,\n",
      " '.': 8,\n",
      " '3': 9,\n",
      " ':': 10,\n",
      " ';': 11,\n",
      " '?': 12,\n",
      " 'A': 13,\n",
      " 'B': 14,\n",
      " 'C': 15,\n",
      " 'D': 16,\n",
      " 'E': 17,\n",
      " 'F': 18,\n",
      " 'G': 19,\n",
      " 'H': 20,\n",
      " 'I': 21,\n",
      " 'J': 22,\n",
      " 'K': 23,\n",
      " 'L': 24,\n",
      " 'M': 25,\n",
      " 'N': 26,\n",
      " 'O': 27,\n",
      " 'P': 28,\n",
      " 'Q': 29,\n",
      " 'R': 30,\n",
      " 'S': 31,\n",
      " 'T': 32,\n",
      " 'U': 33,\n",
      " 'V': 34,\n",
      " 'W': 35,\n",
      " 'X': 36,\n",
      " 'Y': 37,\n",
      " 'Z': 38,\n",
      " 'a': 39,\n",
      " 'b': 40,\n",
      " 'c': 41,\n",
      " 'd': 42,\n",
      " 'e': 43,\n",
      " 'f': 44,\n",
      " 'g': 45,\n",
      " 'h': 46,\n",
      " 'i': 47,\n",
      " 'j': 48,\n",
      " 'k': 49,\n",
      " 'l': 50,\n",
      " 'm': 51,\n",
      " 'n': 52,\n",
      " 'o': 53,\n",
      " 'p': 54,\n",
      " 'q': 55,\n",
      " 'r': 56,\n",
      " 's': 57,\n",
      " 't': 58,\n",
      " 'u': 59,\n",
      " 'v': 60,\n",
      " 'w': 61,\n",
      " 'x': 62,\n",
      " 'y': 63,\n",
      " 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "pp.pprint(tokenizer.char_for_token_id)\n",
    "pp.pprint(tokenizer.token_id_for_char)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:41:59.546063300Z",
     "start_time": "2024-08-06T10:41:59.539542100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "    def __init__(self,data,block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, pos):\n",
    "        assert pos < len(self.data) - self.block_size\n",
    "\n",
    "        x = self.data[pos:pos+self.block_size]\n",
    "        y = self.data[pos+1:pos+1+self.block_size]\n",
    "\n",
    "        return x,y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:41:59.866687Z",
     "start_time": "2024-08-06T10:41:59.859934200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([18, 47, 56,  ..., 45,  8,  0])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenizer.encode(text)\n",
    "tokenized_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:00.333220200Z",
     "start_time": "2024-08-06T10:42:00.209239200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "1115394"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:00.780840100Z",
     "start_time": "2024-08-06T10:42:00.769871600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "dataset = TokenIdsDataset(tokenized_text,block_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:01.264278400Z",
     "start_time": "2024-08-06T10:42:01.254237200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "x,y = dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:01.592795300Z",
     "start_time": "2024-08-06T10:42:01.585153600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50])"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:01.922632600Z",
     "start_time": "2024-08-06T10:42:01.914628900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n        56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,  1,\n        44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1, 57,\n        54, 43, 39, 49,  8,  0,  0, 13, 50, 50])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:02.477895400Z",
     "start_time": "2024-08-06T10:42:02.470689100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAl'"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:03.693792900Z",
     "start_time": "2024-08-06T10:42:03.686301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll'"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:04.057591900Z",
     "start_time": "2024-08-06T10:42:04.052692100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader , RandomSampler\n",
    "\n",
    "sampler = RandomSampler(dataset,replacement=True)\n",
    "dataloader = DataLoader(dataset,batch_size=2,sampler=sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:04.435591800Z",
     "start_time": "2024-08-06T10:42:04.425583900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "x,y = next(iter(dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:05.108632500Z",
     "start_time": "2024-08-06T10:42:05.101152200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 64])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:05.373615600Z",
     "start_time": "2024-08-06T10:42:05.364517400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[57, 43, 52, 45, 43, 56,  1, 61, 46, 53,  1, 40, 47, 42, 57,  1, 40, 43,\n         61, 39, 56, 43,  0, 27, 44,  1, 61, 46, 39, 58,  1, 47, 57,  1, 58, 53,\n          1, 40, 43,  1, 42, 56, 43, 39, 42, 43, 42,  8,  0,  0, 31, 21, 15, 21,\n         26, 21, 33, 31, 10,  0, 32, 43, 50, 50],\n        [50, 47, 41, 43,  8,  0,  0, 30, 21, 34, 17, 30, 31, 10,  0, 31, 39, 61,\n          1, 63, 53, 59,  1, 58, 46, 43,  1, 49, 47, 52, 45,  1, 58, 53,  7, 42,\n         39, 63,  6,  1, 51, 63,  1, 24, 53, 56, 42,  1, 53, 44,  1, 16, 43, 56,\n         40, 63, 12,  0,  0, 16, 17, 30, 14, 37]])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:05.682094400Z",
     "start_time": "2024-08-06T10:42:05.670394400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "'senger who bids beware\\nOf what is to be dreaded.\\n\\nSICINIUS:\\nTell'"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:06.028213300Z",
     "start_time": "2024-08-06T10:42:06.023347500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "'enger who bids beware\\nOf what is to be dreaded.\\n\\nSICINIUS:\\nTell '"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(y[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:06.685122900Z",
     "start_time": "2024-08-06T10:42:06.653367100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "## EMBEDDINGS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:07.063098300Z",
     "start_time": "2024-08-06T10:42:07.059652800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "vocab_size = 10000\n",
    "embedding_dim = 75"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:07.404305500Z",
     "start_time": "2024-08-06T10:42:07.395804900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:07.985035100Z",
     "start_time": "2024-08-06T10:42:07.977137100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "token_ids = torch.LongTensor([11,9783,376,45])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:08.270086700Z",
     "start_time": "2024-08-06T10:42:08.262396600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "embedded = embedding(token_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:08.737328400Z",
     "start_time": "2024-08-06T10:42:08.711273600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 75])"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:09.056248600Z",
     "start_time": "2024-08-06T10:42:09.047833200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "    \"context_size\": 256,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"heads_num\": 12,\n",
    "    \"layers_num\": 10,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"use_bias\": False,\n",
    "}\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:09.841041100Z",
     "start_time": "2024-08-06T10:42:09.828387700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "        self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "        self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "        casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "        self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, tokens_num, embedding_dim = input.shape\n",
    "        Q = self.Q_weights(input)\n",
    "        K = self.K_weights(input)\n",
    "        V = self.V_weights(input)\n",
    "        attention_scores = Q @ K.transpose(1, 2)\n",
    "        attention_scores = attention_scores.masked_fill(\n",
    "            self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "            -torch.inf\n",
    "        )\n",
    "        attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        return attention_scores @ V"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:42:47.217003300Z",
     "start_time": "2024-08-06T10:42:47.211511100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
    "ah = AttentionHead(config)\n",
    "output = ah(input)\n",
    "print(output.shape)  # Expected output: torch.Size([8, 256, 64])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:43:28.322549400Z",
     "start_time": "2024-08-06T10:43:28.276139200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "        self.heads = nn.ModuleList(heads_list)\n",
    "        self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        heads_outputs = [head(input) for head in self.heads]\n",
    "        scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "        scores_change = self.linear(scores_change)\n",
    "        return self.dropout(scores_change)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:43:52.851723900Z",
     "start_time": "2024-08-06T10:43:52.826724700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(config)\n",
    "output = mha(input)\n",
    "print(output.shape)  # Expected output: torch.Size([8, 256, 768])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:44:05.442110800Z",
     "start_time": "2024-08-06T10:44:05.352580200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
    "            nn.Dropout(config[\"dropout_rate\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear_layers(input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:52:13.303123700Z",
     "start_time": "2024-08-06T10:52:13.294182400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "ff = FeedForward(config)\n",
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
    "output = ff(input)\n",
    "print(output.shape)  # Expected: torch.Size([8, 256, 768])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:53:07.175320400Z",
     "start_time": "2024-08-06T10:53:07.100537200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.multi_head = MultiHeadAttention(config)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        residual = input\n",
    "        x = self.multi_head(self.layer_norm_1(input))\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.feed_forward(self.layer_norm_2(x))\n",
    "        return x + residual"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:53:42.807506300Z",
     "start_time": "2024-08-06T10:53:42.799268300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "b = Block(config)\n",
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
    "output = b(input)\n",
    "print(output.shape)  # Expected: torch.Size([8, 256, 768])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T10:54:02.588534800Z",
     "start_time": "2024-08-06T10:54:02.413815400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
    "        self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
    "        blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "        self.layers = nn.Sequential(*blocks)\n",
    "        self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "        self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, tokens_num = token_ids.shape\n",
    "        x = self.token_embedding_layer(token_ids)\n",
    "        sequence = torch.arange(tokens_num, device=device)\n",
    "        x = x + self.positional_embedding_layer(sequence)\n",
    "        x = self.layers(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.unembedding(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:03:01.155826800Z",
     "start_time": "2024-08-06T11:03:01.144883800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 65])\n"
     ]
    }
   ],
   "source": [
    "model = DemoGPT(config).to(device)\n",
    "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))\n",
    "print(output.shape)  # Expected: torch.Size([1, 2, 65])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:03:35.896815400Z",
     "start_time": "2024-08-06T11:03:34.649786300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def generate(model, prompt_ids, max_tokens):\n",
    "    output_ids = prompt_ids\n",
    "    for _ in range(max_tokens):\n",
    "        if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            logits = model(output_ids)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "    return output_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:04:05.357835500Z",
     "start_time": "2024-08-06T11:04:05.349967200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "    model.eval()\n",
    "    prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "    return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:04:13.223923800Z",
     "start_time": "2024-08-06T11:04:13.215586500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "'First Citizen:\\nUyJuEauJBsJb?DyjVl3nxKs GulNK;IpuS?ogqmBBqfTVSvkTVbVftXfZh?koauORHEs,MzSbEhTd.eYNdYMkKTdN&EgLOaO?-YL'"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")\n",
    "# Output: \"First Citizen:\\nDc?SlGAOau$r'QE'us3:nSfYjnkJpFkLVEc$GovIKqlQHxRa\\nA;MWyCrmtZkMp?jCiqNPWFutrO $YcaR:rb!BdGtPyvE$-PVVsE\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:04:38.804260900Z",
     "start_time": "2024-08-06T11:04:36.249454500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(text).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:08:48.767596200Z",
     "start_time": "2024-08-06T11:08:48.613632300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iterations = 5000\n",
    "evaluation_interval = 100\n",
    "learning_rate = 4e-4\n",
    "train_split = 0.9"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:23:35.535964400Z",
     "start_time": "2024-08-06T11:23:35.525459400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(text).to(device)\n",
    "train_count = int(train_split * len(tokenized_text))\n",
    "train_data,validation_data = tokenized_text[:train_count], tokenized_text[train_count:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:24:18.832088200Z",
     "start_time": "2024-08-06T11:24:18.691832200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "train_dataset = TokenIdsDataset(train_data, config[\"context_size\"])\n",
    "validation_dataset = TokenIdsDataset(validation_data, config[\"context_size\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:24:45.994463900Z",
     "start_time": "2024-08-06T11:24:45.990064800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset, num_samples=batch_size * train_iterations, replacement=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "validation_sampler = RandomSampler(validation_dataset,replacement=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, sampler=validation_sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:25:52.217172900Z",
     "start_time": "2024-08-06T11:25:52.207446700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_validation_loss(model, batches_num):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    validation_iter = iter(validation_dataloader)\n",
    "\n",
    "    for _ in range(batches_num):\n",
    "        input, targets = next(validation_iter)\n",
    "        logits = model(input)\n",
    "\n",
    "        logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "        targets_view = targets.view(batch_size * config[\"context_size\"])\n",
    "\n",
    "        loss = F.cross_entropy(logits_view, targets_view)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / batches_num\n",
    "\n",
    "    return average_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:26:42.450855100Z",
     "start_time": "2024-08-06T11:26:42.445234600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:26:43.388338Z",
     "start_time": "2024-08-06T11:26:43.378339600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "    \"context_size\": 256,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"heads_num\": 12,\n",
    "    \"layers_num\": 10,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"use_bias\": False,\n",
    "}\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:14:44.483260600Z",
     "start_time": "2024-08-06T11:14:44.473053200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display,clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:29:45.182000700Z",
     "start_time": "2024-08-06T11:29:42.113775700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecf0978fc7be46019f68db3455c73e16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Loss 2.998\n",
      "Demo GPT:\n",
      "\n",
      "s llG st ssms   lA v l h f shs s s ll s  ls tn f fc   t  s ty nd s s sps s l  s d k s     s h s s d \n",
      "Step 0. Validation loss: 4.019\n",
      "Step 1. Loss 3.984\n",
      "Step 2. Loss 3.741\n",
      "Step 3. Loss 3.518\n",
      "Step 4. Loss 3.281\n",
      "Step 5. Loss 3.155\n",
      "Step 6. Loss 3.058\n",
      "Step 7. Loss 3.090\n",
      "Step 8. Loss 2.941\n",
      "Step 9. Loss 2.848\n",
      "Step 10. Loss 2.827\n",
      "Step 11. Loss 2.793\n",
      "Step 12. Loss 2.769\n",
      "Step 13. Loss 2.726\n",
      "Step 14. Loss 2.698\n",
      "Step 15. Loss 2.688\n",
      "Step 16. Loss 2.643\n",
      "Step 17. Loss 2.658\n",
      "Step 18. Loss 2.641\n",
      "Step 19. Loss 2.622\n",
      "Step 20. Loss 2.610\n",
      "Step 21. Loss 2.605\n",
      "Step 22. Loss 2.586\n",
      "Step 23. Loss 2.588\n",
      "Step 24. Loss 2.585\n",
      "Step 25. Loss 2.574\n",
      "Step 26. Loss 2.545\n",
      "Step 27. Loss 2.565\n",
      "Step 28. Loss 2.564\n",
      "Step 29. Loss 2.548\n",
      "Step 30. Loss 2.547\n",
      "Step 31. Loss 2.547\n",
      "Step 32. Loss 2.532\n",
      "Step 33. Loss 2.539\n",
      "Step 34. Loss 2.505\n",
      "Step 35. Loss 2.523\n",
      "Step 36. Loss 2.524\n",
      "Step 37. Loss 2.526\n",
      "Step 38. Loss 2.535\n",
      "Step 39. Loss 2.506\n",
      "Step 40. Loss 2.490\n",
      "Step 41. Loss 2.495\n",
      "Step 42. Loss 2.520\n",
      "Step 43. Loss 2.500\n",
      "Step 44. Loss 2.502\n",
      "Step 45. Loss 2.502\n",
      "Step 46. Loss 2.483\n",
      "Step 47. Loss 2.497\n",
      "Step 48. Loss 2.510\n",
      "Step 49. Loss 2.471\n",
      "Step 50. Loss 2.489\n",
      "Step 51. Loss 2.490\n",
      "Step 52. Loss 2.492\n",
      "Step 53. Loss 2.493\n",
      "Step 54. Loss 2.467\n",
      "Step 55. Loss 2.486\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[113], line 32\u001B[0m\n\u001B[0;32m     30\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits_view, targets_view)\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Backward propagation\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Update model parameters\u001B[39;00m\n\u001B[0;32m     34\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\.conda\\envs\\sam2\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    523\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\sam2\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m _engine_run_backward(\n\u001B[0;32m    290\u001B[0m     tensors,\n\u001B[0;32m    291\u001B[0m     grad_tensors_,\n\u001B[0;32m    292\u001B[0m     retain_graph,\n\u001B[0;32m    293\u001B[0m     create_graph,\n\u001B[0;32m    294\u001B[0m     inputs,\n\u001B[0;32m    295\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    296\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    297\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\sam2\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    769\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    770\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "plot_output = widgets.Output()\n",
    "display(plot_output)\n",
    "\n",
    "def update_plot(train_losses, train_steps, validation_losses, validation_steps):\n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.plot(train_steps, train_losses, label='Training Loss')\n",
    "        plt.plot(validation_steps, validation_losses, label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='center left')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "train_losses = []\n",
    "train_steps = []\n",
    "eval_losses = []\n",
    "eval_steps = []\n",
    "\n",
    "for step_num, sample in enumerate(train_dataloader):\n",
    "\n",
    "    model.train()\n",
    "    input, targets = sample\n",
    "    logits = model(input)\n",
    "\n",
    "    logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "    targets_view = targets.view(batch_size * config[\"context_size\"])\n",
    "\n",
    "    loss = F.cross_entropy(logits_view, targets_view)\n",
    "    # Backward propagation\n",
    "    loss.backward()\n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "    # Set to None to reduce memory usage\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    train_losses.append((loss.item()))\n",
    "    train_steps.append(step_num)\n",
    "\n",
    "    print(f\"Step {step_num}. Loss {loss.item():.3f}\")\n",
    "\n",
    "    if step_num % evaluation_interval == 0:\n",
    "        print(\"Demo GPT:\\n\" + generate_with_prompt(model, tokenizer, \"\\n\"))\n",
    "        validation_loss = calculate_validation_loss(model,batches_num=10)\n",
    "        eval_losses.append(validation_loss)\n",
    "        eval_steps.append(step_num)\n",
    "\n",
    "        print(f\"Step {step_num}. Validation loss: {validation_loss:.3f}\")\n",
    "\n",
    "    update_plot(train_losses,train_steps,eval_losses,eval_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:43:14.419567900Z",
     "start_time": "2024-08-06T11:34:32.648011900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "prompt = \"During the latest presentation OpenAI\"\n",
    "# prompt = \"During the latest presentation Apple has announced\"\n",
    "model = \"openai-community/gpt2-large\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:09:27.842688400Z",
     "start_time": "2024-08-06T07:09:27.836686600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:09:32.285242Z",
     "start_time": "2024-08-06T07:09:32.279299900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model,device=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:19:40.997335100Z",
     "start_time": "2024-08-06T07:19:38.628897200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\ahmet.yaylalioglu\\.conda\\envs\\sam2\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the latest presentation OpenAI CEO and co-founder Demis Hassabis took to the stage again to introduce the new project, which is the culmination of years of thinking about and planning. Hassabis said that the two main goals for OpenAI at this time are \"to create a platform that gives rise to a new class of AI researchers, which then leads to creating AI that can actually solve real problems\" and \"to empower AI in the public sphere with the first step toward creating more secure AI\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "generated_texts = text_generator(prompt, max_length=100, truncation=True, num_return_sequences=1)\n",
    "\n",
    "print(generated_texts[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:19:50.609690500Z",
     "start_time": "2024-08-06T07:19:48.359005800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "do_sample=False\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI's CEO, John Robb, said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\n",
      "\n",
      "Robb also said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\n",
      "\n",
      "\"We're going to be a big deal,\" he said. \"We're going to be the biggest AI project ever.\"\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "do_sample=True\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI's AI chief and former Google DeepMind CEO Demis Hassabis outlined the future of AI with its latest \"AlphaGo\" artificial general intelligence software and argued against the notion of human-level intelligence  as currently envisioned at Google DeepMind by artificial intelligence researcher Alan Turing  on the basis that the ability to mimic the way humans think and interact with one another was the true benchmark for artificial intelligence capabilities.\n",
      "\n",
      "He also argued there was too much focus on human-level\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for do_sample in [\n",
    "    False, # Greedy Search\n",
    "    True   # Multinomial sampling\n",
    "]:\n",
    "    generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=do_sample, num_beams=1)\n",
    "\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Parameters:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"do_sample={do_sample}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Generation:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(generated_texts[0]['generated_text'])\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:20:12.865172200Z",
     "start_time": "2024-08-06T07:20:09.666241800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "num_beams=1\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI's CEO, John Robb, said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\n",
      "\n",
      "Robb also said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\n",
      "\n",
      "\"We're going to be a big deal,\" he said. \"We're going to be the biggest AI project ever.\"\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "num_beams=2\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI's CEO and co-founder Demis Hassabis said that the company is working on a new type of artificial intelligence that will be able to learn from its mistakes.\n",
      "\n",
      "\"We're working on a new type of artificial intelligence that will learn from its mistakes,\" Hassabis said. \"We're not going to be able to predict the future, but we're going to be able to learn from our mistakes.\"\n",
      "\n",
      "Hassabis said that the company is working\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "num_beams=4\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI's CEO and co-founder Demis Hassabis said, \"We are excited to announce that we have reached an agreement with DeepMind to work together on the development of the next generation of artificial intelligence.\"\n",
      "\n",
      "DeepMind is a British artificial intelligence company founded in 2012. It is best known for its work on AlphaGo, a computer program that defeated the world's best Go player, Lee Sedol, at the end of 2016.\n",
      "\n",
      "\"We are excited\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "num_beams=8\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI's CEO and co-founder Demis Hassabis said:\n",
      "\n",
      "\"We've been working on this for a long time. We've been working on this for a long time. We've been working on this for a long time. We've been working on this for a long time. We've been working on this for a long time. We've been working on this for a long time. We've been working on this for a long time. We've\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beam-search strategy\n",
    "\n",
    "for beams in [1, 2, 4, 8]:\n",
    "    generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=False, num_beams=beams)\n",
    "\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Parameters:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"num_beams={beams}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Generation:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(generated_texts[0]['generated_text'])\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:20:36.299913200Z",
     "start_time": "2024-08-06T07:20:29.253737600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "num_beams=1\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI and others held yesterday to discuss their work involving a neural network and a convolutional layer of a super-deep neural network, they made several suggestions and criticisms of the work. In doing so, they showed two important things; First, that deep learning systems have gotten significantly better, compared to just a few years ago. Second, that today's most sophisticated techniques could indeed be taken beyond simple reinforcement learning and into more complex tasks, such as image recognition and text generation\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "num_beams=2\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI, a San Francisco-based artificial intelligence company, showed off its latest project, DeepMind's AlphaGo, a computer program that beat the world's top Go player, Lee Sedol, in a six-game match in March.\n",
      "\n",
      "AlphaGo's victory came after a series of complex games, in which the program learned to recognize patterns and play the game at a faster pace than humans.\n",
      "\n",
      "\"This is a tremendous achievement for DeepMind,\" said Deep\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "num_beams=4\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI CEO Demis Hassabis revealed that the team at the company has been working on a new AI system that will be used in the game.\n",
      "\n",
      "\"We have been working on a new AI system, which will be used in the game,\" said Hassabis.\n",
      "\n",
      "\"We have been working on a new AI system, which will be used in the game,\" said Hassabis. \"We have been working on a new AI system, which will be used in\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "num_beams=8\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI CEO Demis Hassabis said, \"This is the beginning of a new era for artificial intelligence, and we're excited to be a part of it.\"\n",
      "\n",
      "OpenAI is a non-profit organization that aims to advance the state-of-the-art in artificial intelligence and machine learning.\n",
      "\n",
      "OpenAI is a non-profit organization that aims to advance the state-of-the-art in artificial intelligence and machine learning.\n",
      "\n",
      "OpenAI is\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beam-search multinomial sampling\n",
    "\n",
    "for beams in [1, 2, 4, 8]:\n",
    "    generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=beams)\n",
    "\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Parameters:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"num_beams={beams}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Generation:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(generated_texts[0]['generated_text'])\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:20:49.369511900Z",
     "start_time": "2024-08-06T07:20:42.164211400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "top_k=1\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI's CEO, John Robb, said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\n",
      "\n",
      "Robb also said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\n",
      "\n",
      "\"We're going to be a big deal,\" he said. \"We're going to be the biggest AI project ever.\"\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "top_k=5\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI has shown the ability to create a self-driving car that can navigate around the streets of San Francisco. The self-driving cars have a range of about 30 miles, and they're being tested on public roads. It's a big improvement on the previous self-driving car that was shown in a video that OpenAI released in March. That video was shot by Google's self-driving car team, and it was a pretty impressive demonstration of what the team is capable\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "top_k=10\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI had been working with a team of researchers from Google Brain to develop a computer-vision system that can detect objects at a distance, and that can recognize the shape and orientation of objects on the screen without having to know the object's shape. That is a huge step forward for the field.\n",
      "\n",
      "This research will allow AI to make better decisions about objects on a scene. The system is able to recognize things such as objects with multiple faces, or objects that are very\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "top_k=50\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI is hosting, a young computer scientist by the name of Ilya Sutskever, will be answering questions about the project. A similar video about the project is expected to be posted soon, but if you haven't already seen it, head over to the youtube.com/openai channel and watch. It's a pretty good look at what the team is actually making.\n",
      "\n",
      "All in all, it's good to see that they're not really holding us\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "top_k=100\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI has demonstrated using a three human brains encased in a neural network to perform AI tasks, such as understanding speech and playing music.\n",
      "\n",
      "The neural network would then attempt to guess the theme song that would be played next and then automatically play it. Of course, the goal isn't to beat humans at an art contest, but to make AI machines that can perform meaningful tasks.\n",
      "\n",
      "The whole thing is a little difficult and involves lots of images showing \"mice\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "top_k=500\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI, where each speaker contributes to his own computer (making not this a debate but science), David Clarke and others answered these questions.\n",
      "\n",
      "Why self-contained systems?\n",
      "\n",
      "With Watson  a system, not a person  people expect sophisticated AI systems. To handle intense workloads and rapid changes, a single cognitive processing unit (or cPU) maintains accurate records of a wide variety of \"things.\" Accessible to all researchers, it should provide manageable datasets in\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the top-k parameter\n",
    "\n",
    "for k in [1, 5, 10, 50, 100, 500]:\n",
    "    generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, top_k=k)\n",
    "\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Parameters:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"top_k={k}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Generation:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(generated_texts[0]['generated_text'])\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:21:06.166500600Z",
     "start_time": "2024-08-06T07:20:56.882327600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "temperature=0.1\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI's CEO, John Robb, said that the company had been working on the project for a year and a half. He also said that the company had been working on the project for a year and a half.\n",
      "\n",
      "Robb also said that the company had been working on the project for a year and a half. He also said that the company had been working on the project for a year and a half.\n",
      "\n",
      "\"We have been working on this project for\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "temperature=1.0\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI and Facebook announced that they have teamed up and that OpenAI software will be used in the development of DeepDream, Facebook's artificial intelligence software that can create original 3D images of the environment.\n",
      "\n",
      "That's very big news and very exciting news. I was told by OpenAI's founder, Yishai Stein, that it was the biggest breakthrough in deep learning since the inception of the company in 2011.\n",
      "\n",
      "While that's not entirely the case, we\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "temperature=2.0\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI sent for an invitation, a slide made out to an upcoming article about deep learning and computer vision revealed the future of VR. Not on the HTC HMD they are also making it possible for people living in future times -- even tomorrow time -- to see what happens after events happened and even live events on this device like Super Bowl 51 or The Simpsons finale. If your kids want to join VR adventures playing Mario Maker while hanging out in the backyard but in reality they just ran\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "Parameters:\n",
      "-----------------------------------\n",
      "temperature=3.0\n",
      "-----------------------------------\n",
      "Generation:\n",
      "-----------------------------------\n",
      "During the latest presentation OpenAI also explained a new kind of intelligence model that does one key things. AI cannot simulate behavior at real size by trying a model on one part; what it has to manage properly then involves running models for multiple complex problems while making incremental tweaks and adding weight or tweaking individual elements of the data sets. But as explained, this model will have several big advantages because it is fast  perhaps a minute each frame! Furthermore any adjustments one made now can apply dynamically as they occur during\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change temperature\n",
    "\n",
    "for temp in [0.1, 1.0, 2.0, 3.0]:\n",
    "    generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, temperature=temp)\n",
    "\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Parameters:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"temperature={temp}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Generation:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(generated_texts[0]['generated_text'])\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:21:23.945966100Z",
     "start_time": "2024-08-06T07:21:17.575785300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
